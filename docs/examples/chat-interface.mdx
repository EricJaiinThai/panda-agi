---
title: 'Chat Interface Example'
description: 'Build a real-time chat interface with streaming agent responses'
---

## Overview

This example demonstrates how to build a modern chat interface that streams AI agent responses in real-time. The interface shows agent thinking, tool usage, and provides a smooth user experience.

<img
  className="block dark:hidden"
  src="/images/chat_interface.png"
  alt="Chat Interface"
/>
<img
  className="hidden dark:block"
  src="/images/chat_interface.png"
  alt="Chat Interface"
/>

## 🚀 Ready-to-Use Implementation

A complete, enhanced chat interface implementation is already available in the examples folder! You can get started immediately with our Docker-based setup.

### Quick Start

```bash
cd examples/ui && ./start.sh
```

This one command will:
- ✅ Check Docker requirements  
- 🛑 Stop any existing containers
- 🔨 Build and start both services
- 🔍 Perform health checks
- 📊 Display service status and URLs

Once running, access:
- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8001

### Enhanced Features

The implemented solution includes:

<CardGroup cols={2}>
  <Card
    title="Docker Integration"
    icon="docker"
  >
    One-command deployment with Docker Compose
  </Card>
  <Card
    title="Enhanced Event Handling"
    icon="gears"
  >
    Improved event filtering and error handling
  </Card>
  <Card
    title="Production Ready"
    icon="shield-check"
  >
    Nginx serving, proper CORS, health checks
  </Card>
  <Card
    title="Mobile Responsive"
    icon="mobile"
  >
    Optimized for both desktop and mobile devices
  </Card>
</CardGroup>

### Alternative Setup

If you prefer manual setup or local development:

```bash
# Backend
cd examples/ui/backend && python main.py

# Frontend (in new terminal)
cd examples/ui/frontend && npm start
```

## Features

<CardGroup cols={2}>
  <Card
    title="Real-time Streaming"
    icon="bolt"
  >
    Live updates as the agent processes requests
  </Card>
  <Card
    title="Event Visualization"
    icon="eye"
  >
    Visual indicators for different event types
  </Card>
  <Card
    title="Tool Tracking"
    icon="wrench"
  >
    Show which tools the agent is using
  </Card>
  <Card
    title="Modern UI"
    icon="palette"
  >
    Beautiful, responsive design with Tailwind CSS
  </Card>
</CardGroup>

## Backend Implementation

### FastAPI Server

You can wrap the Agent in a simple backend server with streaming support:

```python
# backend/main.py
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import json
import asyncio
from typing import AsyncGenerator
import uuid

from panda_agi.client import Agent
from panda_agi.client.models import EventType
from panda_agi.envs import DockerEnv

app = FastAPI(title="PandaAGI Chat API")

# Enable CORS for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str
    conversation_id: str = None

async def event_stream(query: str, conversation_id: str = None) -> AsyncGenerator[str, None]:
    """Stream agent events as Server-Sent Events"""
    agent = None
    try:
        # Create environment for the agent
        agent_env = DockerEnv("./workspace")
        
        # Create agent with conversation ID for session persistence
        agent = Agent(
            environment=agent_env,
            conversation_id=conversation_id or str(uuid.uuid4())
        )
        
        # Stream events from the agent
        async for event in agent.run(query):
            # Convert event to dictionary for JSON serialization
            event_data = {
                "type": event.type.value,
                "timestamp": event.timestamp,
                "data": event.data
            }
            
            # Format as Server-Sent Event
            yield f"data: {json.dumps(event_data)}\n\n"
            
            # Add small delay to prevent overwhelming the client
            await asyncio.sleep(0.05)
            
    except Exception as e:
        # Send error event
        error_event = {
            "type": "error",
            "timestamp": "",
            "data": {"error": str(e)}
        }
        yield f"data: {json.dumps(error_event)}\n\n"
        
    finally:
        # Clean up agent connection
        if agent:
            try:
                await agent.disconnect()
            except Exception as e:
                print(f"Error disconnecting agent: {e}")

@app.post("/api/chat/stream")
async def stream_chat(request: ChatRequest):
    """Stream chat responses using Server-Sent Events"""
    
    return StreamingResponse(
        event_stream(request.message, request.conversation_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
        }
    )

@app.get("/api/health")
async def health_check():
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### Requirements

```txt
# backend/requirements.txt
fastapi==0.104.1
uvicorn==0.24.0
panda-agi>=0.1.0
pydantic==2.5.0
```


## Key Features Demonstrated

- **Real-time streaming**: See agent events as they happen
- **Event visualization**: Different icons and formatting for each event type
- **Session persistence**: Conversation ID maintains context across messages
- **Modern UI**: Responsive design with smooth animations
- **Error handling**: Graceful error display and recovery

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Basic Usage Example"
    icon="play"
    href="/examples/basic-usage"
  >
    Learn the fundamentals with a simple example
  </Card>
  <Card
    title="Custom Tools Example"
    icon="wrench"
    href="/examples/custom-tools"
  >
    Build custom tools for your agents
  </Card>
</CardGroup> 