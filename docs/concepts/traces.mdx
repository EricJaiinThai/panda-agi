---
title: "Traces"
description: "Debug and analyze your LLM calls with PandaAGI traces"
---

# Traces - Capture and Analyze LLM API Interactions

<Info>
  PandaAGI's tracing functionality lets you observe, collect, and debug LLM API
  calls made through popular providers like OpenAI, Anthropic, and LiteLLM.
</Info>

## Introduction

When developing AI applications, understanding exactly what happens during LLM API calls is essential for debugging, optimization, and cost management. PandaAGI's `traces` module provides powerful tools to intercept API calls, collect detailed metrics, and present insights about these interactions.

The core component of this module is the `observe` utility, which can be used as both a decorator and a context manager.

## The `observe` Utility

The `observe` class is designed to:

- Intercept LLM API calls
- Collect detailed information about requests and responses
- Track usage metrics (tokens, costs)
- Measure latency and performance
- Print summaries of collected data

### Usage Patterns

#### As a Decorator

The decorator pattern is perfect for monitoring entire functions that make LLM API calls:

```python
from panda_agi.traces import observe

@observe(providers=["openai"], model_name="gpt-4")
async def generate_content():
    # Your code making API calls
    response = await openai.ChatCompletion.acreate(
        model="gpt-4",
        messages=[{"role": "user", "content": "Explain quantum computing"}]
    )
    return response.choices[0].message.content
```

#### As a Context Manager

The context manager pattern is useful for monitoring specific sections of your code:

```python
from panda_agi.traces import observe

async def process_data():
    # Pre-processing code

    # Only trace the LLM calls
    with observe(providers=["openai", "anthropic"]):
        response1 = await openai.ChatCompletion.acreate(...)
        response2 = await anthropic.Completion.create(...)

    # Post-processing code
```

### Configuration Options

The `observe` utility accepts several parameters to customize its behavior:

| Parameter    | Type                  | Description                                                                                                                                        |
| ------------ | --------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model_name` | `Optional[str]`       | A tag or identifier for the model being used. This doesn't override the model specified in the API call but serves as a label in the trace output. |
| `providers`  | `Optional[List[str]]` | List of providers to monitor (`"openai"`, `"anthropic"`, `"litellm"`). If not specified, all available providers are monitored.                    |
| `debug`      | `Optional[bool]`      | When set to `True`, provides more detailed output including full request and response objects. Default is `False`.                                 |

## Supported Providers

PandaAGI traces currently support the following LLM providers:

<CardGroup cols={3}>
  <Card title="OpenAI" icon="robot">
    Both v0 and v1 API formats
  </Card>
  <Card title="Anthropic" icon="a">
    Claude and Claude Instant models
  </Card>
  <Card title="LiteLLM" icon="sparkles">
    Universal provider that supports multiple backends
  </Card>
</CardGroup>

## Example: Model Performance Analysis

One powerful application is comparing performance metrics across different models:

```python
import asyncio
import openai
from panda_agi.traces import observe

async def compare_models():
    # Define test prompt
    prompt = "Explain the theory of relativity in simple terms"

    # Test with GPT-3.5
    with observe(model_name="gpt-3.5-turbo", providers=["openai"]):
        await openai.ChatCompletion.acreate(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )

    # Test with GPT-4
    with observe(model_name="gpt-4", providers=["openai"]):
        await openai.ChatCompletion.acreate(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )

if __name__ == "__main__":
    asyncio.run(compare_models())
```

In this example, the `model_name` parameter serves as a tag that helps you clearly identify which trace belongs to which model when reviewing the output.

## Real-World Use Cases

### Debugging Complex Workflows

When your application has complex chains of LLM calls, traces help you understand exactly what's happening at each step:

```python
from panda_agi.traces import observe

@observe(debug=True)
async def content_generation_pipeline():
    # Stage 1: Generate outline
    outline = await generate_outline(topic)

    # Stage 2: Expand each section
    sections = []
    for item in outline:
        section = await expand_section(item)
        sections.append(section)

    # Stage 3: Final polish
    final_content = await polish_content(sections)

    return final_content
```

### Cost Optimization

Trace LLM API usage to identify opportunities for optimization:

```python
with observe(providers=["openai"]):
    # Run your application code
    await my_app_function()

# Analyze the output to find:
# - Excessive token usage
# - Redundant API calls
# - Slow response times
```

### A/B Testing

Compare different prompt structures or models to determine which performs best:

```python
# Test Prompt A
with observe(model_name="prompt-a"):
    response_a = await openai.ChatCompletion.acreate(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt_a}]
    )

# Test Prompt B
with observe(model_name="prompt-b"):
    response_b = await openai.ChatCompletion.acreate(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt_b}]
    )
```

Here, the `model_name` parameter is used to tag each experiment, making it easy to distinguish between the two in the output.

## Advanced Usage: Custom Data Processing

For advanced usage, you can access the collected data programmatically:

```python
from panda_agi.traces import observe

async def analyze_call_patterns():
    # Setup the observer
    observer = observe(providers=["openai"])

    # Use as context manager
    with observer:
        # Your code making API calls
        await perform_multiple_calls()

    # Access collected data after the context exits
    for proxy in observer.active:
        if hasattr(proxy, "collected_data"):
            # Calculate average response time
            total_time = sum(item.get("duration", 0) for item in proxy.collected_data)
            avg_time = total_time / len(proxy.collected_data)

            # Calculate total token usage
            total_tokens = sum(
                item.get("usage", {}).get("total_tokens", 0)
                for item in proxy.collected_data
            )

            print(f"Average response time: {avg_time:.2f}s")
            print(f"Total tokens used: {total_tokens}")
```

## Integration with Other Tools

The traces module works seamlessly with other components of the PandaAGI ecosystem:

```python
from panda_agi import Agent, Knowledge
from panda_agi.envs import LocalEnv
from panda_agi.traces import observe
from panda_agi.handlers import LogsHandler

@observe(providers=["openai", "anthropic"])
async def agent_workflow():
    # Create and configure agent
    agent = Agent(
        environment=LocalEnv("./workspace"),
        event_handlers=[LogsHandler(use_colors=True)]
    )

    # Run agent tasks
    response = await agent.run("Research quantum computing advancements")

    return response
```

## Performance Considerations

While the traces module is extremely useful for development and debugging, be aware of these considerations for production use:

- There is a small performance overhead for intercepting and collecting data
- With `debug=True`, memory usage increases due to storing complete request/response objects
- For high-throughput applications, consider using sampling instead of tracing every call

## Conclusion

The `panda_agi.traces` module, particularly the `observe` utility, provides deep insights into LLM API interactions. By using it as either a decorator or context manager, you can capture valuable data to help debug issues, optimize costs, and improve the overall performance of your AI applications.

Remember that the `model_name` parameter serves as a convenient tagging mechanism, allowing you to label different traces for easier identification and comparison, even when using the same actual model in your API calls.
