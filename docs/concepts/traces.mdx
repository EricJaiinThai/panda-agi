---
title: "Traces"
description: "Debug and analyze your LLM calls with PandaAGI traces"
---

# Traces - Capture and Analyze LLM API Interactions

<Info>
  PandaAGI's tracing functionality lets you observe, collect, and debug LLM API
  calls made through popular providers like OpenAI, Anthropic, and LiteLLM.
</Info>

## Introduction

When developing AI applications, understanding exactly what happens during LLM API calls is essential for debugging, optimization, and cost management. PandaAGI's `traces` module provides powerful tools to intercept API calls, collect detailed metrics, and present insights about these interactions.

The core component of this module is the `observe` utility, which can be used as both a decorator and a context manager.

## The `observe` Utility

The `observe` class is designed to:

- Intercept LLM API calls
- Collect detailed information about requests and responses
- Track usage metrics (tokens, costs)
- Measure latency and performance

### Usage Patterns

#### As a Decorator

The decorator pattern is perfect for monitoring entire functions that make LLM API calls:

```python
from panda_agi.traces import observe

@observe(providers=["openai"], model_name="openai_train")
async def generate_content():
    # Your code making API calls
    response = await openai.ChatCompletion.acreate(
        model="gpt-4",
        messages=[{"role": "user", "content": "Explain quantum computing"}]
    )
    return response.choices[0].message.content
```

#### As a Context Manager

The context manager pattern is useful for monitoring specific sections of your code:

```python
from panda_agi.traces import observe

async def process_data():
    # Pre-processing code

    # Only trace the LLM calls
    with observe(providers=["openai", "anthropic"]):
        response1 = await openai.ChatCompletion.acreate(...)
        response2 = await anthropic.Completion.create(...)

    # Post-processing code
```

### Configuration Options

The `observe` utility accepts several parameters to customize its behavior:

| Parameter    | Type                  | Description                                                                                                                                        |
| ------------ | --------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model_name` | `Optional[str]`       | A tag or identifier for the model being used. This doesn't override the model specified in the API call but serves as a label in the trace output. |
| `providers`  | `Optional[List[str]]` | List of providers to monitor (`"openai"`, `"anthropic"`, `"litellm"`). If not specified, all available providers are monitored.                    |
| `debug`      | `Optional[bool]`      | When set to `True`, provides more detailed output including full request and response objects. Default is `False`.                                 |

## Supported Providers

PandaAGI traces currently support the following LLM providers:

<CardGroup cols={3}>
  <Card title="OpenAI" icon="robot">
    Both v0 and v1 API formats
  </Card>
  <Card title="Anthropic" icon="a">
    Claude and Claude Instant models
  </Card>
  <Card title="LiteLLM" icon="sparkles">
    Universal provider that supports multiple backends
  </Card>
</CardGroup>

## Conclusion

The `panda_agi.traces` module, particularly the `observe` utility, provides deep insights into LLM API interactions. By using it as either a decorator or context manager, you can capture valuable data to help debug issues, optimize costs, and improve the overall performance of your AI applications.

Remember that the `model_name` parameter serves as a convenient tagging mechanism, allowing you to label different traces for easier identification and comparison, even when using the same actual model in your API calls.
